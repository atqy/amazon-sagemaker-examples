{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fea2d2-dd26-41dd-b1b7-a72aeef5d979",
   "metadata": {},
   "source": [
    "# Scale interactive experimentation to scheduled jobs on SageMaker Studio Notebooks without changing code.\n",
    "\n",
    "In addition to the the interactive ML experience that [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) provides, data workers also seek solutions to execute notebooks as batch jobs without the need to refactor code as python modules and without having to learn DevOps tools and best practices to automate and manage their deployment infrastructure. Some common usecases for doing this include:\n",
    "\n",
    "* Executing model inference regularly to generate reports\n",
    "* Scaling up a feature engineering step after having tested in Studio against a small subset of data on a small instance\n",
    "* Re-training and deploying models on events of new data arrivals\n",
    "\n",
    "SageMaker Studio provides an built-in extension for you to run your notebooks as-is or in a parameterized fashion. You can run these notebooks on a schedule or immedietely with the run-now capability. There's no need for the end user to modify their existing notebook code. \n",
    "\n",
    "![arch](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b13d70-2b5a-40c3-8bb2-e3c88050cf7f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "In order to utilize the SageMaker Job Scheduling extension you'll need to have JupyterLab3 enabled for your Studio IDE. More information on this process can be found [in the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jl.html#studio-jl-view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15a285-f230-4f1f-85c5-aa4e91778fb8",
   "metadata": {},
   "source": [
    "## Example Background\n",
    "\n",
    "### Customer Churn Use Case\n",
    "Losing customers is costly for any business. For our scheduled notebook workflow, we're going to use a familiar example of customer churn: leaving a mobile phone operator. If a provider knows that a customer is thinking of leaving, it can offer timely incentives and the customer may stick around. Incentives are often much more cost-effective than losing and reacquiring a customer.\n",
    "\n",
    "\n",
    "### Solution Overview\n",
    "We're going to build an end to end workflow which will train a refreshed machine learning model on recent data and then generate a daily report. The process will be as follows:\n",
    "\n",
    "1. Gather \"the last two weeks\" of carrier data. In production, this may be from a data lake or feature store.\n",
    "2. Process that data in order to train a refreshed machine learning model. \n",
    "3. Train a scikit-learn RandomForest model on the previous data\n",
    "4. Plot the Confusion matrix and F1 score of our refreshed model so that we can understand it's effectiveness on known data\n",
    "5. Run inference using our refreshed model on \"todays\" recent data to determine which accounts are at risk of leaving the carrier\n",
    "6. Schedule this notebook to execute every day.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b39e8d-17eb-4181-996f-b068bfae6db1",
   "metadata": {},
   "source": [
    "## Parameterize Notebook\n",
    "Using notebook cell metadata, we can mark a cell with the tag \"parameters\" if we want to modify any variables during automated execution. For this example, we'll specify default hyperparamter values which can be modified for our notebook job. To mark a cell as parameter inputs, simply select the wheel icon on the right panel and add a \"parameters\" tag:\n",
    "\n",
    "![params](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/parameters.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a56e2c-ff90-4b38-af78-1f4eefae1b16",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# These parameters can be modified when executing this notebook as a job\n",
    "number_rf_estimators = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed22481-9b0b-4cd3-b05f-9bc9e4f0dc31",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "In this example we'll use a synthetic dataset from a Telecommunications company that we'll use to predict whether or not a particular customer will leave or churn. For a more in-depth exploratory data analysis, [view this example ](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f5ef9-b0c2-485a-af9b-0ab69bebdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download additional assets for your use in your notebook jobs\n",
    "!aws s3 cp s3://sagemaker-sample-files/datasets/scripts/sagemaker-studio-scheduling/synthetic_data.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd05807-6050-426e-8184-56e9986ff516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from synthetic_data import generate_data\n",
    "\n",
    "\n",
    "previous_two_weeks_data = generate_data(5000, label_known=True)\n",
    "todays_data = generate_data(300, label_known=False)\n",
    "\n",
    "previous_two_weeks_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb97db-0766-42d2-ac30-e2df36112284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, label_known):\n",
    "    \"\"\"\n",
    "    This function represents typical data munging that needs to be performed for machine learning workflows.\n",
    "    We'll cast data types, drop columns that are irrelevant as features to the model, and convert categorical\n",
    "    features into indicator variables. If the data is not from the ground truth dataset, we'll drop it's churn label.\n",
    "    \"\"\"\n",
    "    df = df.drop(\"Phone\", axis=1)\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "    df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "    model_data = pd.get_dummies(df)\n",
    "\n",
    "    if label_known:\n",
    "        model_data = pd.concat(\n",
    "            [\n",
    "                model_data[\"Churn?_True.\"],\n",
    "                model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        model_data = model_data.rename(columns={\"Churn?_True.\": \"Churn\"})\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e0062-7c74-47aa-a702-991449278ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_prior_data = process_data(previous_two_weeks_data, label_known=True)\n",
    "processed_todays_data = process_data(todays_data, label_known=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41162bb7-ae71-4415-bf42-e4cfb7d7d3b6",
   "metadata": {},
   "source": [
    "## Train a refreshed model \n",
    "\n",
    "In our example, we're going to train a refreshed model on the previous 2 weeks of data. In production, you may wish to train a refreshed model or even fine-tune an existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79e95e-2428-4822-9d17-65edca7de422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y = np.ravel(processed_prior_data[[\"Churn\"]])\n",
    "x = processed_prior_data.drop([\"Churn\"], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=int(number_rf_estimators), criterion=\"gini\")\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066407a2-0d05-4a54-85df-c449091a935f",
   "metadata": {},
   "source": [
    "## Generate predictions from the refreshed model on our validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58bd1d-51ba-44b8-a7f0-a5a6b3516a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on test set\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "md(\"## Refreshed Model's F1 Score on validation data: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea9c29-3afe-4a49-add0-3e61dfedbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test set results\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Churn\", \"Didn't Churn\"])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3df66b-a374-43b2-a159-b2bb148872bb",
   "metadata": {},
   "source": [
    "## Run inference on today's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72a457-fb08-460d-9ac9-261c9ab815c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_predictions = clf.predict(processed_todays_data)\n",
    "accounts_likely_to_churn = pd.concat(\n",
    "    [pd.DataFrame(todays_predictions, columns=[\"Predicted Churn\"]), processed_todays_data], axis=1\n",
    ")\n",
    "accounts_likely_to_churn = accounts_likely_to_churn[\n",
    "    accounts_likely_to_churn[\"Predicted Churn\"] == 1\n",
    "]\n",
    "\n",
    "accounts_likely_to_churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97e336-a53c-4e15-b35d-cddb889d0df6",
   "metadata": {},
   "source": [
    "## Store predictions\n",
    "\n",
    "In a production setting, you may want to store these predictions into a database such as DynamoDB or trigger a corresponding action based on the account-ids. Alternatively, you may want to register this particular model into SageMaker Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa91532-20f3-4b8d-abc1-1061ed444900",
   "metadata": {},
   "source": [
    "## Run the Notebook as a job to visualize how the daily report will look\n",
    "\n",
    "1. Right click your notebook and select \"Create Notebook Job\"\n",
    "\n",
    "![create job](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/create_job.png)\n",
    "\n",
    "1. Alternatively you can select the \"Notebook job\" icon on your notebook bar\n",
    "\n",
    "![job_icon](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/job_icon.png)\n",
    "\n",
    "2. Choose the right instance type for your scheduled job based on your workload - Standard Instances, Compute Optimized Instances or Accelerated computing instances that contain GPUs. You can choose any of the instances available for SageMaker training jobs. See here for the complete list of instances available: https://aws.amazon.com/sagemaker/pricing/\n",
    "\n",
    "3. You can expand the \"Additional Options\" to modify any other settings. SageMaker Studio will automatically detect the Image/Kernel you are using in your notebook and will pre-select it for you. Ensure that you have validated this selection.\n",
    "\n",
    "4. You can enter a new value for the `number_rf_estimators` if you'd like\n",
    "\n",
    "![options](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/options.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5752e75-2e75-4f31-88b5-ba551050c906",
   "metadata": {},
   "source": [
    "## Schedule Notebook as a Job for Daily Reports\n",
    "\n",
    "To run this self contained notebook as a daily scheduled job, we can simply use the built-in functionality available in SageMaker Studio. You may need to add permissions to your SageMaker execution role. More information can be found [in the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/scheduled-notebook-policies.html).\n",
    "\n",
    "\n",
    "1. Simply click the \"Run on a schedule\" button and set your desired schedule\n",
    "\n",
    "![schedule](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/schedule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c534a8-5cce-4a0a-b856-03b2032d0a36",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "If you scheduled this notebook, be sure to delete your schedule job definition once your experimentation is complete.\n",
    "\n",
    "![definitions](https://sagemaker-sample-files.s3.amazonaws.com/images/sagemaker-studio-scheduling/job_definitions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23056db2-214d-46e1-b1be-ba8f6d0c98e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}